output_path = '~/Desktop/entrepreneurship_research/sentiment_analysis/output/logreg_summary.txt'
){
for(c in comment_sentiments){
print(c)
start_time <- proc.time()
d <- cbind(reddit_data[features], response = reddit_data[[c]])
d$response <- as.factor(ifelse(d$response >= median(d$response), 1, 0))
scale_features <- features[!features %in% c("gender_post","gender_comment", "subreddit", "author_premium", "is_self")]
d <- d %>% mutate_at(scale_features, ~(scale(.) %>% as.vector))
lr_model <- glm(response~., family = binomial, data = d)
predictions_prob <- predict(lr_model, type="response")
predictions_class <- as.factor(ifelse(predictions_prob>0.5, 1, 0))
print(proc.time()-start_time)
cat(paste0("\n",c,"\n"), file = output_path, append = TRUE)
capture.output(summary(lr_model), file = output_path, append = TRUE)
capture.output(confusionMatrix(predictions_class, d$response), file = output_path, append = TRUE)
ROCit_obj <- rocit(score = predictions_prob, class = d$response)
png(filename=paste0("~/Desktop/entrepreneurship_research/sentiment_analysis/output/plots/roc_logreg_",c,".png"))
plot(ROCit_obj)
dev.off()
rm(lr_model, predictions_prob, predictions_class)
}
}
reddit_path <- "~/Desktop/entrepreneurship_research/sentiment_analysis/src/reddit_data.csv"
input <- load_reddit(reddit_path)
reddit_data <- input[[1]]
features <- input[[2]]
comment_sentiments <- input[[3]]
rm(input)
fit_logreg(reddit_data, features, comment_sentiments)
# library(e1071)
# c <- 'trust_comment'
# d <- cbind(reddit_data[features], response = reddit_data[[c]])
# d$response <- as.factor(ifelse(d$response >= median(d$response), 1, 0))
# svmfit <- svm(response~., data = d, kernel = "linear", cost = 10)
fit_logreg <- function(
reddit_data,
features,
comment_sentiments,
output_path = '~/Desktop/entrepreneurship_research/sentiment_analysis/output/logreg_summary.txt'
){
for(c in comment_sentiments){
print(c)
start_time <- proc.time()
d <- cbind(reddit_data[features], response = reddit_data[[c]])
d$response <- as.factor(ifelse(d$response >= median(d$response), 1, 0))
print(str(d))
scale_features <- features[!features %in% c("gender_post","gender_comment", "subreddit", "author_premium", "is_self")]
d <- d %>% mutate_at(scale_features, ~(scale(.) %>% as.vector))
lr_model <- glm(response~., family = binomial, data = d)
predictions_prob <- predict(lr_model, type="response")
predictions_class <- as.factor(ifelse(predictions_prob>0.5, 1, 0))
print(proc.time()-start_time)
cat(paste0("\n",c,"\n"), file = output_path, append = TRUE)
capture.output(summary(lr_model), file = output_path, append = TRUE)
capture.output(confusionMatrix(predictions_class, d$response), file = output_path, append = TRUE)
ROCit_obj <- rocit(score = predictions_prob, class = d$response)
png(filename=paste0("~/Desktop/entrepreneurship_research/sentiment_analysis/output/plots/roc_logreg_",c,".png"))
plot(ROCit_obj)
dev.off()
rm(lr_model, predictions_prob, predictions_class)
}
}
fit_logreg(reddit_data, features, comment_sentiments)
fit_logreg <- function(
reddit_data,
features,
comment_sentiments,
output_path = '~/Desktop/entrepreneurship_research/sentiment_analysis/output/logreg_summary.txt'
){
for(c in comment_sentiments){
print(c)
start_time <- proc.time()
d <- cbind(reddit_data[features], response = reddit_data[[c]])
d$response <- as.factor(ifelse(d$response >= median(d$response), 1, 0))
# print(str(d))
scale_features <- features[!features %in% c("gender_post","gender_comment", "subreddit", "author_premium", "is_self","no_follow")]
d <- d %>% mutate_at(scale_features, ~(scale(.) %>% as.vector))
lr_model <- glm(response~., family = binomial, data = d)
predictions_prob <- predict(lr_model, type="response")
predictions_class <- as.factor(ifelse(predictions_prob>0.5, 1, 0))
print(proc.time()-start_time)
cat(paste0("\n",c,"\n"), file = output_path, append = TRUE)
capture.output(summary(lr_model), file = output_path, append = TRUE)
capture.output(confusionMatrix(predictions_class, d$response), file = output_path, append = TRUE)
ROCit_obj <- rocit(score = predictions_prob, class = d$response)
png(filename=paste0("~/Desktop/entrepreneurship_research/sentiment_analysis/output/plots/roc_logreg_",c,".png"))
plot(ROCit_obj)
dev.off()
rm(lr_model, predictions_prob, predictions_class)
}
}
fit_logreg(reddit_data, features, comment_sentiments)
library(randomForest)
library(ROCit)
library(caret)
library(dplyr)
load_reddit <- function(reddit_path){
reddit_data <- read.csv(reddit_path)
reddit_data <- reddit_data[complete.cases(reddit_data),]
cols <- colnames(reddit_data)
features <- cols[grepl("_post", cols) | grepl("gender_", cols) | cols == "subreddit"]
features <- c(features, 'post_wordcount', 'title_wordcount', 'comment_wordcount', 'ups', 'author_premium',
'is_self', 'no_follow', 'num_comments')
comment_sentiments <- cols[grepl("_comment", cols) & !grepl("gender", cols)]
return(list(reddit_data, features, comment_sentiments))
}
fit_linear_reg <- function(
reddit_data,
features,
comment_sentiments
){
output_path <- "~/Desktop/entrepreneurship_research/sentiment_analysis/output/model_summary_agg.txt"
for(c in comment_sentiments){
print(c)
d <- cbind(reddit_data[features], response = reddit_data[[c]])
lm_fit <- lm(response~. + gender_post * gender_comment, data = d)
cat(paste0("\n",c,"\n"), file = output_path, append = TRUE)
capture.output(summary(lm_fit), file = output_path, append = TRUE)
}
}
fit_random_forest <- function(
reddit_data,
features,
comment_sentiments
){
output_path <- '~/Desktop/entrepreneurship_research/sentiment_analysis/output/rf50_scaled_summary.txt'
for(c in comment_sentiments){
print(c)
start_time <- proc.time()
d <- cbind(reddit_data[features], response = reddit_data[[c]])
d$response <- as.factor(ifelse(d$response >= median(d$response), 1, 0))
rf_model <- randomForest(
response ~ .,
data = d,
importance = TRUE,
ntree = 50,
mtry = 5
)
print(proc.time()-start_time)
cat(paste0("\n",c,"\n"), file = output_path, append = TRUE)
capture.output(rf_model, file = output_path, append = TRUE)
capture.output(importance(rf_model), file = output_path, append = TRUE)
# varImpPlot(rf_model)
predictions <- predict(rf_model, d, type = 'prob')
ROCit_obj <- rocit(score=predictions[,2],class=d$response)
png(filename=paste0("~/Desktop/entrepreneurship_research/sentiment_analysis/output/plots/roc_rf50_scaled_",c,".png"))
plot(ROCit_obj)
dev.off()
rm(rf_model, predictions)
}
}
fit_logreg <- function(
reddit_data,
features,
comment_sentiments,
output_path = '~/Desktop/entrepreneurship_research/sentiment_analysis/output/logreg_summary.txt'
){
for(c in comment_sentiments){
print(c)
start_time <- proc.time()
d <- cbind(reddit_data[features], response = reddit_data[[c]])
d$response <- as.factor(ifelse(d$response >= median(d$response), 1, 0))
# print(str(d))
scale_features <- features[!features %in% c("gender_post","gender_comment", "subreddit", "author_premium", "is_self","no_follow")]
d <- d %>% mutate_at(scale_features, ~(scale(.) %>% as.vector))
lr_model <- glm(response~., family = binomial, data = d)
predictions_prob <- predict(lr_model, type="response")
predictions_class <- as.factor(ifelse(predictions_prob>0.5, 1, 0))
print(proc.time()-start_time)
cat(paste0("\n",c,"\n"), file = output_path, append = TRUE)
capture.output(summary(lr_model), file = output_path, append = TRUE)
capture.output(confusionMatrix(predictions_class, d$response), file = output_path, append = TRUE)
ROCit_obj <- rocit(score = predictions_prob, class = d$response)
png(filename=paste0("~/Desktop/entrepreneurship_research/sentiment_analysis/output/plots/roc_logreg_",c,".png"))
plot(ROCit_obj)
dev.off()
rm(lr_model, predictions_prob, predictions_class)
}
}
reddit_path <- "~/Desktop/entrepreneurship_research/sentiment_analysis/src/reddit_data.csv"
input <- load_reddit(reddit_path)
reddit_data <- input[[1]]
features <- input[[2]]
comment_sentiments <- input[[3]]
rm(input)
fit_random_forest(reddit_data, features, comment_sentiments)
# library(e1071)
# c <- 'trust_comment'
# d <- cbind(reddit_data[features], response = reddit_data[[c]])
# d$response <- as.factor(ifelse(d$response >= median(d$response), 1, 0))
# svmfit <- svm(response~., data = d, kernel = "linear", cost = 10)
library(randomForest)
library(ROCit)
library(caret)
library(dplyr)
load_reddit <- function(reddit_path){
reddit_data <- read.csv(reddit_path)
reddit_data <- reddit_data[complete.cases(reddit_data),]
cols <- colnames(reddit_data)
features <- cols[grepl("_post", cols) | grepl("gender_", cols) | cols == "subreddit"]
features <- c(features, 'post_wordcount', 'title_wordcount', 'comment_wordcount', 'ups', 'author_premium',
'is_self', 'no_follow', 'num_comments')
comment_sentiments <- cols[grepl("_comment", cols) & !grepl("gender", cols)]
return(list(reddit_data, features, comment_sentiments))
}
fit_linear_reg <- function(
reddit_data,
features,
comment_sentiments
){
output_path <- "~/Desktop/entrepreneurship_research/sentiment_analysis/output/model_summary_agg.txt"
for(c in comment_sentiments){
print(c)
d <- cbind(reddit_data[features], response = reddit_data[[c]])
lm_fit <- lm(response~. + gender_post * gender_comment, data = d)
cat(paste0("\n",c,"\n"), file = output_path, append = TRUE)
capture.output(summary(lm_fit), file = output_path, append = TRUE)
}
}
fit_random_forest <- function(
reddit_data,
features,
comment_sentiments
){
output_path <- '~/Desktop/entrepreneurship_research/sentiment_analysis/output/rf50_scaled_summary.txt'
for(c in comment_sentiments){
print(c)
start_time <- proc.time()
d <- cbind(reddit_data[features], response = reddit_data[[c]])
d$response <- as.factor(ifelse(d$response >= median(d$response), 1, 0))
scale_features <- features[!features %in% c("gender_post","gender_comment", "subreddit", "author_premium", "is_self","no_follow")]
d <- d %>% mutate_at(scale_features, ~(scale(.) %>% as.vector))
rf_model <- randomForest(
response ~ .,
data = d,
importance = TRUE,
ntree = 50,
mtry = 5
)
print(proc.time()-start_time)
cat(paste0("\n",c,"\n"), file = output_path, append = TRUE)
capture.output(rf_model, file = output_path, append = TRUE)
capture.output(importance(rf_model), file = output_path, append = TRUE)
# varImpPlot(rf_model)
predictions <- predict(rf_model, d, type = 'prob')
ROCit_obj <- rocit(score=predictions[,2],class=d$response)
png(filename=paste0("~/Desktop/entrepreneurship_research/sentiment_analysis/output/plots/roc_rf50_scaled_",c,".png"))
plot(ROCit_obj)
dev.off()
rm(rf_model, predictions)
}
}
fit_logreg <- function(
reddit_data,
features,
comment_sentiments,
output_path = '~/Desktop/entrepreneurship_research/sentiment_analysis/output/logreg_summary.txt'
){
for(c in comment_sentiments){
print(c)
start_time <- proc.time()
d <- cbind(reddit_data[features], response = reddit_data[[c]])
d$response <- as.factor(ifelse(d$response >= median(d$response), 1, 0))
# print(str(d))
scale_features <- features[!features %in% c("gender_post","gender_comment", "subreddit", "author_premium", "is_self","no_follow")]
d <- d %>% mutate_at(scale_features, ~(scale(.) %>% as.vector))
lr_model <- glm(response~., family = binomial, data = d)
predictions_prob <- predict(lr_model, type="response")
predictions_class <- as.factor(ifelse(predictions_prob>0.5, 1, 0))
print(proc.time()-start_time)
cat(paste0("\n",c,"\n"), file = output_path, append = TRUE)
capture.output(summary(lr_model), file = output_path, append = TRUE)
capture.output(confusionMatrix(predictions_class, d$response), file = output_path, append = TRUE)
ROCit_obj <- rocit(score = predictions_prob, class = d$response)
png(filename=paste0("~/Desktop/entrepreneurship_research/sentiment_analysis/output/plots/roc_logreg_",c,".png"))
plot(ROCit_obj)
dev.off()
rm(lr_model, predictions_prob, predictions_class)
}
}
reddit_path <- "~/Desktop/entrepreneurship_research/sentiment_analysis/src/reddit_data.csv"
input <- load_reddit(reddit_path)
reddit_data <- input[[1]]
features <- input[[2]]
comment_sentiments <- input[[3]]
rm(input)
fit_random_forest(reddit_data, features, comment_sentiments)
# library(e1071)
# c <- 'trust_comment'
# d <- cbind(reddit_data[features], response = reddit_data[[c]])
# d$response <- as.factor(ifelse(d$response >= median(d$response), 1, 0))
# svmfit <- svm(response~., data = d, kernel = "linear", cost = 10)
c <- 'negative_comment'
d <- cbind(reddit_data[c(features, "parent_id")], response = reddit_data[[c]])
# Data type conversion.
d$gender_comment <- as.factor(d$gender_comment)
d$gender_post <- as.factor(d$gender_post)
plot(d$gender_comment, d$response)
plot(d$gender_comment[d$comment_wordcount<20], d$response[d$comment_wordcount<20])
library(ggplot2)
ggplot(d[d$comment_wordcount<20], aes(x=gender_comment, y=response, fill=gender_comment)) +
geom_boxplot()
ggplot(d[d$comment_wordcount<20,], aes(x=gender_comment, y=response, fill=gender_comment)) +
geom_boxplot()
ggplot(d[d$comment_wordcount<20,], aes(response, fill = gender_comment)) + geom_density(alpha = 0.2)
# Why does removing short comments change the trend?
# Let's plot negative_comment against gender_comment
ggplot(d[d$comment_wordcount<20,], aes(x=gender_comment, y=response, fill=gender_comment)) +
geom_boxplot()
ggplot(d[d$comment_wordcount<20,], aes(response, fill = gender_comment)) + geom_density(alpha = 0.2)
plot(d$gender_comment, d$comment_wordcount)
plot(d$comment_wordcount, d$response)
plot(d$comment_wordcount[d$response<0.4], d$response[d$response<0.4])
setwd("~/Desktop/reddit/src/")
reddit_path <- "../data/reddit_data.csv"
reddit_data <- read.csv(reddit_path)
View(reddit_data)
reddit_data$created_utc[2:3]
setwd("~/Desktop/reddit/src/")
reddit_path <- "../data/reddit_data.csv"
reddit_data <- read.csv(reddit_path)
View(reddit_data)
reddit_data[2,]
reddit_data[3,]
for(c in colnames(reddit_data)){
if(reddit_data[2,c]!=reddit_data[3,c])
print(c)
}
setwd("~/Desktop/reddit/src/")
reddit_path <- "../data/reddit_data.csv"
reddit_data <- read.csv(reddit_path)
View(reddit_data)
View(reddit_data)
reddit_data[reddit_data$link_id=="t3_8mucde",]
setwd("~/Desktop/reddit/src/")
reddit_path <- "../data/reddit_data.csv"
reddit_data <- read.csv(reddit_path)
# for(c in colnames(reddit_data)){
#         if(reddit_data[2,c]!=reddit_data[3,c])
#                 print(c)
# }
View(reddit_data)
reddit_data[reddit_data$link_id=="t3_8mucde",]
len(unique(reddit_data$id))
length(unique(reddit_data$id))
n_occur <- data.frame(table(reddit_data$id))
n_occur[n_occur$Freq > 1,]
reddit_data[reddit_data$id=="faf45qf",]
for(c in colnames(reddit_data)){
if(reddit_data[102107,c]!=reddit_data[168449,c])
print(c)
}
setwd("~/Desktop/reddit/src/")
reddit_path <- "../data/reddit_data.csv"
reddit_data <- read.csv(reddit_path)
View(reddit_data)
length(unique(reddit_data$id))
n_occur <- data.frame(table(reddit_data$id))
View(n_occur)
n_occur[n_occur$Freq > 1,]
setwd("~/Desktop/reddit/src/")
reddit_path <- "../data/reddit_data.csv"
reddit_data <- read.csv(reddit_path)
cols <- colnames(reddit_data)
features <- cols[grepl("_post", cols) | grepl("gender_", cols)]
features <- c(features, 'subreddit', 'post_wordcount', 'title_wordcount', 'comment_wordcount', 'ups', 'author_premium',
'is_self', 'no_follow', 'num_comments')
features
comment_sentiments <- cols[grepl("_comment", cols) & !grepl("gender", cols)]
comment_sentiments
comment_sentiments <- cols[grepl("_comment", cols) & !grepl("gender", cols) & !(cols == 'num_comments')]
cols
comment_sentiments
setwd("~/Desktop/reddit/src/")
# Load the merged reddit post and comment data.
load_reddit <- function(reddit_path="../data/reddit_data.csv"){
reddit_data <- read.csv(reddit_path)
cols <- colnames(reddit_data)
features <- cols[grepl("_post", cols) | grepl("gender_", cols)]
features <- c(features, 'subreddit', 'post_wordcount', 'title_wordcount', 'comment_wordcount', 'ups', 'author_premium',
'is_self', 'no_follow', 'num_comments')
comment_sentiments <- cols[grepl("_comment", cols) & !grepl("gender", cols) & !(cols == 'num_comments')]
return(list(reddit_data, features, comment_sentiments))
}
reddit_path <- "../output/reddit_data.csv"
input <- load_reddit(reddit_path)
reddit_data <- input[[1]]
features <- input[[2]]
comment_sentiments <- input[[3]]
rm(input)
setwd("~/Desktop/reddit/src/")
# Load the merged reddit post and comment data.
load_reddit <- function(reddit_path="../data/reddit_data.csv"){
reddit_data <- read.csv(reddit_path)
cols <- colnames(reddit_data)
features <- cols[grepl("_post", cols) | grepl("gender_", cols)]
features <- c(features, 'subreddit', 'post_wordcount', 'title_wordcount', 'comment_wordcount', 'ups', 'author_premium',
'is_self', 'no_follow', 'num_comments')
comment_sentiments <- cols[grepl("_comment", cols) & !grepl("gender", cols) & !(cols == 'num_comments')]
return(list(reddit_data, features, comment_sentiments))
}
reddit_path <- "../data/reddit_data.csv"
input <- load_reddit(reddit_path)
reddit_data <- input[[1]]
features <- input[[2]]
comment_sentiments <- input[[3]]
rm(input)
# Linear Mixed Models
# Let's subset the data for one comment sentiment.
c <- 'trust_comment'
d <- cbind(reddit_data[c(features, "link_id")], response = reddit_data[[c]])
is.numeric(4)
is.numeric(c(1,2))
library(ggplot2)
# Now, let's plot histograms of all variables.
# Will do log transforms where distribution is skewed.
plot_histograms(d, features)
plot_histograms <- function(d, features){
is_numeric <- sapply(features, FUN = function(c) {return(is.numeric(d[[c]]))})
hist_features <- features[!is_numeric]
for(h in hist_features){
print(paste0("../plots/hist_plots/", h, ".png"))
png(filename = paste0("../plts/hist_plots/", h, ".png"))
print(ggplot(d, aes_(x=as.name(h))) + geom_histogram(color="darkblue", fill="lightblue", bins = 50))
dev.off()
}
}
# Now, let's plot histograms of all variables.
# Will do log transforms where distribution is skewed.
plot_histograms(d, features)
plot_histograms <- function(d, features){
is_numeric <- sapply(features, FUN = function(c) {return(is.numeric(d[[c]]))})
hist_features <- features[is_numeric]
for(h in hist_features){
print(paste0("../plots/hist_plots/", h, ".png"))
png(filename = paste0("../plts/hist_plots/", h, ".png"))
print(ggplot(d, aes_(x=as.name(h))) + geom_histogram(color="darkblue", fill="lightblue", bins = 50))
dev.off()
}
}
# Now, let's plot histograms of all variables.
# Will do log transforms where distribution is skewed.
plot_histograms(d, features)
plot_histograms <- function(d, features){
is_numeric <- sapply(features, FUN = function(c) {return(is.numeric(d[[c]]))})
hist_features <- features[is_numeric]
for(h in hist_features){
print(paste0("../plots/hist_plots/", h, ".png"))
png(filename = paste0("../plots/hist_plots/", h, ".png"))
print(ggplot(d, aes_(x=as.name(h))) + geom_histogram(color="darkblue", fill="lightblue", bins = 50))
dev.off()
}
}
# Now, let's plot histograms of all variables.
# Will do log transforms where distribution is skewed.
plot_histograms(d, features)
transform_features <- function(d, features){
is_numeric <- sapply(features, FUN = function(c) {return(is.numeric(d[[c]]))})
skewed_features <- features[is_numeric]
for(s in skewed_features){
d[[s]] <- log(1+d[[s]] - min(d[[s]]))
}
return(d)
}
d <- transform_features(d, features)
# Now, let's plot histograms of all variables.
# Will do log transforms where distribution is skewed.
plot_histograms(reddit_data, features)
c <- 'trust_comment'
d <- cbind(reddit_data[c(features, "link_id")], response = reddit_data[[c]])
d <- transform_features(d, features)
library(randomForest)
library(ROCit)
library(caret)
library(dplyr)
library(lme4)
library(ggplot2)
c <- 'trust_comment'
d <- cbind(reddit_data[c(features, "link_id")], response = reddit_data[[c]])
d <- transform_features(d, features)
# Making sure our features are scaled!
d <- scale_features(d, features)
scale_features <- function(d, features){
is_numeric <- sapply(features, FUN = function(c) {return(is.numeric(d[[c]]))})
scale_f <- features[is_numeric]
d <- d %>% mutate_at(scale_f, ~(scale(.) %>% as.vector))
return(d)
}
# Making sure our features are scaled!
d <- scale_features(d, features)
colnames(d)
# Finally, building an LMER model.
mixed.lmer <- lmer(response ~ . + predicted_gender_comment:comment_wordcount +
trust_post:predicted_gender_post +
trust_post:predicted_gender_comment +
fairness_post:predicted_gender_post +
fairness_post:predicted_gender_comment +
predicted_gender_post:predicted_gender_comment -
link_id +
(1|link_id), data = d)
# Diagnostics
summary(mixed.lmer)
# Plot residuals vs fitted values.
plot(mixed.lmer)
# Plot residuals vs fitted values.
plot(mixed.lmer)
summary(mixed.lmer)
# Plot residuals vs fitted values.
plot(mixed.lmer)
# Plot residuals vs fitted values.
plot(mixed.lmer)
